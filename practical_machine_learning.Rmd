---
title: "Machine Learning Prediction Assignment"
author: "Raheel"
date: "15/06/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---

# Background

>In this project, the  goal is to use data from accelerometers of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways.
> Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.
For more detail of the data for this project the souce is: http://groupware.les.inf.puc-rio.br/har.
> The goal of the project is to predict the manner in which the participants did the exercise. This is the "classe" variable in the training set and the predicted model is applied to predict 20 different test cases.  

---

### Loading necessary Libraries
```{r, eval=TRUE}
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(corrplot)
library(rattle)
library(randomForest)
library(RColorBrewer)
library(e1071) 
set.seed(4321)
```

### Loading Data sets
```{r}
traindata_url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testdata_url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train_data <- read.csv(url(traindata_url), strip.white = TRUE, na.strings = c("NA",""))
test_data  <- read.csv(url(testdata_url),  strip.white = TRUE, na.strings = c("NA",""))
```

## Exploring Data

```{r}
dim(train_data)
```

#### Creating data partition for training and testing data
```{r}
in_train  <- createDataPartition(train_data$classe, p=0.75, list=FALSE)
train_set <- train_data[ in_train, ]
test_set  <- train_data[-in_train, ]
dim(train_set)
dim(test_set)
```

#### Removing the large number of NA values in both the train_set and test_set
```{r}
nzv_var <- nearZeroVar(train_set)
train_set <- train_set[ , -nzv_var]
test_set  <- test_set [ , -nzv_var]
dim(train_set)
dim(test_set)
```

> The variable are decreased from 160 to 119

> Now removing the variables that are mostly NA. A threshlod of 95 % is selected.

```{r}
na_var <- sapply(train_set, function(x) mean(is.na(x))) > 0.95
train_set <- train_set[ , na_var == FALSE]
test_set  <- test_set [ , na_var == FALSE]
dim(train_set)
dim(test_set)
```

> The variables are further decreased from 119 to 59.

>> Since the columns 1 to 5 are identification variables only so they are to be removed as well.

```{r}
train_set <- train_set[ , -(1:5)]
test_set  <- test_set [ , -(1:5)]
dim(train_set)
dim(test_set)
```

### Correlation Analysis
>> Perform a correlation analysis between the variables before the modeling work itself is done. Select “FPC” for the first principal component order.

```{r}
corr_matrix <- cor(train_set[ , -54])
corrplot(corr_matrix, order = "FPC", method = "circle", type = "lower",
         tl.cex = 0.6, tl.col = rgb(0, 0, 0))
```

>>If two variables are highly correlated their colors are either dark blue (for a positive correlation) or dark red (for a negative corraltions).
>>To further reduce the number of variables, a Principal Components Analysis (PCA) could be performed as the next step. However, since there are only very few strong correlations among the input variables,the PCA will not be performed. Instead, a few different prediction models will be built next.

# Prediction Models
## Decision Tree Model
```{r}
set.seed(1813)
fit_decision_tree <- rpart(classe ~ ., data = train_set, method="class")
fancyRpartPlot(fit_decision_tree)
```

### Predictions of the decision tree model on test_set.
```{r}
predict_decision_tree <- predict(fit_decision_tree, newdata = test_set, type="class")
conf_matrix_decision_tree <- confusionMatrix(predict_decision_tree, test_set$classe)
conf_matrix_decision_tree
```

>>The predictive accuracy of the decision tree model is relatively low at 74.9 %.

#### Plot the predictive accuracy of the decision tree model.
```{r}
plot(conf_matrix_decision_tree$table, col = conf_matrix_decision_tree$byClass,
     main = paste("Decision Tree Model: Predictive Accuracy =",
                  round(conf_matrix_decision_tree$overall['Accuracy'], 4)))
```

## Support Vector Machine
```{r}
SVM = svm(formula = classe ~ .,
          data = train_set,
          type = 'C-classification',
          kernel = 'linear')

predict_SVM <- predict(SVM, newdata = test_set) 
conf_matrix_SVM <- confusionMatrix(predict_SVM, test_set$classe) 
conf_matrix_SVM 
```

>> The predictive accuracy of SVM model is still pretty low at 78.3 %.

#### Plot the predictive accuracy of the SVM model.
```{r}
plot(conf_matrix_SVM$table, col = conf_matrix_SVM$byClass,
     main = paste("SVM Model: Predictive Accuracy =",
                  round(conf_matrix_SVM$overall['Accuracy'], 4)))

```

## Random Forest Model
```{r}
set.seed(1813)
ctrl_RF <- trainControl(method = "repeatedcv", number = 5, repeats = 2)
fit_RF  <- train(classe ~ ., data = train_set, method = "rf",
                 trControl = ctrl_RF, verbose = FALSE)
fit_RF$finalModel
```

#### Predictions of the Random Forest model on test_set.
```{r}
predict_RF <- predict(fit_RF, newdata = test_set)
conf_matrix_RF <- confusionMatrix(predict_RF, test_set$classe)
conf_matrix_RF
```
>>The predictive accuracy of the Random Forest model is excellent at 99.8 %.

> The best predictive model is to be applied to the Test Data. The Random Forest model is selected and applied to make predictions on the 20 data points from the original testing dataset (data_quiz).

## Results on Test Set

```{r}
predict_quiz <- predict(fit_RF, newdata = test_data)
predict_quiz
```



